{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the WordCloud library\n",
    "\n",
    "I've chosen to use this library because there is no need to re-invent the wheel. Most naive implementations using bare matplotlib have superimposed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T22:23:27.545519Z",
     "start_time": "2019-10-25T22:23:21.149701Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from utils import read_mongo, json_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the db from mongodb\n",
    "df = read_mongo('dbTweets', 'tweets_chile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(dataframe, wordlist):\n",
    "    \"\"\"\n",
    "    Count how many tweets contain a given word\n",
    "    :param dataframe: Pandas dataframe from the tweepy mining\n",
    "    :param wordlist: array-like with the keywords\n",
    "    \n",
    "    TODO: - drop dependency on numpy?\n",
    "    \"\"\"\n",
    "    word_freq = dict()\n",
    "    for word in wordlist:\n",
    "        word_freq[word] = np.where(df['text'].str.contains(word))[0].size\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = io.open('kw.csv').read()\n",
    "wordlist = wordlist.split(', ')\n",
    "wf = get_word_frequency(df, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla example of WordCloud functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate_from_frequencies(wf)\n",
    "plt.imshow(word_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make this bigger and with nicer colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(background_color='white', colormap='plasma', width= 1200, height=800).generate_from_frequencies(wf)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.axis('off')\n",
    "ax.imshow(word_cloud, interpolation='bicubic');\n",
    "plt.savefig('wordcloud-test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Constants\n",
    "img_width = 1600\n",
    "img_height = 900\n",
    "scale_factor = 0.5\n",
    "\n",
    "# Add invisible scatter trace.\n",
    "# This trace is added to help the autoresize logic work.\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, img_width * scale_factor],\n",
    "        y=[0, img_height * scale_factor],\n",
    "        mode=\"markers\",\n",
    "        marker_opacity=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Configure axes\n",
    "fig.update_xaxes(\n",
    "    visible=False,\n",
    "    range=[0, img_width * scale_factor]\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    visible=False,\n",
    "    range=[0, img_height * scale_factor],\n",
    "    # the scaleanchor attribute ensures that the aspect ratio stays constant\n",
    "    scaleanchor=\"x\"\n",
    ")\n",
    "\n",
    "# Add image\n",
    "fig.update_layout(\n",
    "    images=[go.layout.Image(\n",
    "        x=0,\n",
    "        sizex=img_width * scale_factor,\n",
    "        y=img_height * scale_factor,\n",
    "        sizey=img_height * scale_factor,\n",
    "        xref=\"x\",\n",
    "        yref=\"y\",\n",
    "        opacity=1.0,\n",
    "        layer=\"below\",\n",
    "        sizing=\"stretch\",\n",
    "        source=Image.fromarray(word_cloud.to_array()))]\n",
    ")\n",
    "\n",
    "\n",
    "# Configure other layout\n",
    "fig.update_layout(\n",
    "    width=img_width * scale_factor,\n",
    "    height=img_height * scale_factor,\n",
    "    margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n",
    ")\n",
    "\n",
    "# Disable the autosize on double click because it adds unwanted margins around the image\n",
    "# More detail: https://plot.ly/python/configuration-options/\n",
    "fig.show(config={'doubleClick': 'reset'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what I need is a function that receives a dataframe and returns a PIL Image object with the wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_keywords\n",
    "kw = get_keywords()\n",
    "def create_wordcloud_raster(dataframe, wordlist,\n",
    "                            wc_kwargs=dict(background_color='white', colormap='plasma', width= 1200, height=800)):\n",
    "    \"\"\"\n",
    "    Generate a wordcloud of the keywords given, wheighted by the number of \n",
    "    unique tweets they appear in.\n",
    "    :param dataframe: Pandas DataFrame object. It must contain a 'text' column with the\n",
    "    tweets from the stream.\n",
    "    :param wordlist: list of strings to plot in the word cloud.\n",
    "    :param wc_kwargs: dict of keyword arguments to give to the WordCloud\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "    wf = get_word_frequency(dataframe, wordlist)\n",
    "    word_cloud = WordCloud(**wc_kwargs).generate_from_frequencies(wf)\n",
    "    return Image.fromarray(word_cloud.to_array())\n",
    "\n",
    "create_wordcloud_raster(df, kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count(string):\n",
    "    return Counter(string.split())\n",
    "    \n",
    "count_map_list = list(map(count, df['tweet'][:10]))\n",
    "master_count = sum(count_map_list, Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "simple_preprocess(df['tweet'][0], min_len=5,max_len=40, deacc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pprint\n",
    "import gensim.parsing as gp\n",
    "from gensim.utils import deaccent\n",
    "\n",
    "with open('stopwords-es.txt', 'r') as file:\n",
    "    stopwords = list()\n",
    "    for line in file:\n",
    "        stop_word = deaccent(line.strip('\\n'))\n",
    "        stopwords.append(stop_word)\n",
    "stopwords = frozenset(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OutputStreaming_20191101-171445.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tw_handles = r\"([@][A-z]+)|([#][A-z]+)\"\n",
    "urls = r\"((\\w+:\\/\\/)[-a-zA-Z0-9:@;?&=\\/%\\+\\.\\*!'\\(\\),\\$_\\{\\}\\^~\\[\\]`#|]+)\"\n",
    "multi_pattern = '|'.join([tw_handles, urls])\n",
    "non_plain_re = re.compile(multi_pattern, re.UNICODE)\n",
    "\n",
    "def remove_non_plain(document):\n",
    "    return non_plain_re.sub('', document)\n",
    "\n",
    "\n",
    "def strip_punctuation(token):\n",
    "    return deaccent(token).strip('\",.:;?¿-()[]<>!¡“”|*/\\=+&$% ')\n",
    "\n",
    "doc = df['text'][11]\n",
    "# filtered_tokenization = [strip_punctuation(word) for word in remove_non_plain(doc).lower().split() if word not in stopwords]\n",
    "# print(filtered_tokenization)\n",
    "\n",
    "def process(doc):\n",
    "    wordbag = list()\n",
    "    for token in remove_non_plain(doc).lower().split():\n",
    "        token = strip_punctuation(token)\n",
    "        if token not in stopwords and token != '' and token != 'rt':\n",
    "            wordbag.append(token)\n",
    "    return set(wordbag)\n",
    "\n",
    "documents = map(process, df['text'])\n",
    "tfs = Counter()\n",
    "for document in documents:\n",
    "    tfs.update(document)\n",
    "\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# dct = Dictionary(documents)\n",
    "# tfs= dict()\n",
    "# for token in dct.values():\n",
    "#     id_ = dct.token2id[token]\n",
    "#     tfs[token] = dct.cfs[id_]\n",
    "\n",
    "# tfs = Counter(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud().generate_from_frequencies(tfs)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from npl_utils import process, init_counter\n",
    "df = json_pandas(read_mongo('dbTweets', 'tweets_chile',\n",
    "                           query_fields={\"dateTweet\":1, \"tweet\":1, \"screenName\":1},\n",
    "                           json_only=True, num_limit=10 ** 5))\n",
    "ctmain = Counter()\n",
    "twiterator = map(process, df['tweet'])\n",
    "for wordbag in twiterator:\n",
    "    ctmain.update(wordbag)\n",
    "\n",
    "ctmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctmain.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import strip_punctuation, strip_punctuation2\n",
    "strip_punctuation2('23197(&**Ŷ*((*jhj,sdhja<>?/|œh👽')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df['tweet'][999])\n",
    "pprint('----')\n",
    "pprint(Counter(set(process(df['tweet'][999]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
