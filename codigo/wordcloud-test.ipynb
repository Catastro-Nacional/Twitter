{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the WordCloud library\n",
    "\n",
    "I've chosen to use this library because there is no need to re-invent the wheel. Most naive implementations using bare matplotlib have superimposed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T22:23:27.545519Z",
     "start_time": "2019-10-25T22:23:21.149701Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from utils import read_mongo, json_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the db from mongodb\n",
    "df = read_mongo('dbTweets', 'tweets_chile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(dataframe, wordlist):\n",
    "    \"\"\"\n",
    "    Count how many tweets contain a given word\n",
    "    :param dataframe: Pandas dataframe from the tweepy mining\n",
    "    :param wordlist: array-like with the keywords\n",
    "    \n",
    "    TODO: - drop dependency on numpy?\n",
    "    \"\"\"\n",
    "    word_freq = dict()\n",
    "    for word in wordlist:\n",
    "        word_freq[word] = np.where(df['text'].str.contains(word))[0].size\n",
    "    \n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = io.open('kw.csv').read()\n",
    "wordlist = wordlist.split(', ')\n",
    "wf = get_word_frequency(df, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla example of WordCloud functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud().generate_from_frequencies(wf)\n",
    "plt.imshow(word_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make this bigger and with nicer colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(background_color='white', colormap='plasma', width= 1200, height=800).generate_from_frequencies(wf)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.axis('off')\n",
    "ax.imshow(word_cloud, interpolation='bicubic');\n",
    "plt.savefig('wordcloud-test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Constants\n",
    "img_width = 1600\n",
    "img_height = 900\n",
    "scale_factor = 0.5\n",
    "\n",
    "# Add invisible scatter trace.\n",
    "# This trace is added to help the autoresize logic work.\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, img_width * scale_factor],\n",
    "        y=[0, img_height * scale_factor],\n",
    "        mode=\"markers\",\n",
    "        marker_opacity=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Configure axes\n",
    "fig.update_xaxes(\n",
    "    visible=False,\n",
    "    range=[0, img_width * scale_factor]\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    visible=False,\n",
    "    range=[0, img_height * scale_factor],\n",
    "    # the scaleanchor attribute ensures that the aspect ratio stays constant\n",
    "    scaleanchor=\"x\"\n",
    ")\n",
    "\n",
    "# Add image\n",
    "fig.update_layout(\n",
    "    images=[go.layout.Image(\n",
    "        x=0,\n",
    "        sizex=img_width * scale_factor,\n",
    "        y=img_height * scale_factor,\n",
    "        sizey=img_height * scale_factor,\n",
    "        xref=\"x\",\n",
    "        yref=\"y\",\n",
    "        opacity=1.0,\n",
    "        layer=\"below\",\n",
    "        sizing=\"stretch\",\n",
    "        source=Image.fromarray(word_cloud.to_array()))]\n",
    ")\n",
    "\n",
    "\n",
    "# Configure other layout\n",
    "fig.update_layout(\n",
    "    width=img_width * scale_factor,\n",
    "    height=img_height * scale_factor,\n",
    "    margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n",
    ")\n",
    "\n",
    "# Disable the autosize on double click because it adds unwanted margins around the image\n",
    "# More detail: https://plot.ly/python/configuration-options/\n",
    "fig.show(config={'doubleClick': 'reset'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what I need is a function that receives a dataframe and returns a PIL Image object with the wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_keywords\n",
    "kw = get_keywords()\n",
    "def create_wordcloud_raster(dataframe, wordlist,\n",
    "                            wc_kwargs=dict(background_color='white', colormap='plasma', width= 1200, height=800)):\n",
    "    \"\"\"\n",
    "    Generate a wordcloud of the keywords given, wheighted by the number of \n",
    "    unique tweets they appear in.\n",
    "    :param dataframe: Pandas DataFrame object. It must contain a 'text' column with the\n",
    "    tweets from the stream.\n",
    "    :param wordlist: list of strings to plot in the word cloud.\n",
    "    :param wc_kwargs: dict of keyword arguments to give to the WordCloud\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "    wf = get_word_frequency(dataframe, wordlist)\n",
    "    word_cloud = WordCloud(**wc_kwargs).generate_from_frequencies(wf)\n",
    "    return Image.fromarray(word_cloud.to_array())\n",
    "\n",
    "create_wordcloud_raster(df, kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count(string):\n",
    "    return Counter(string.split())\n",
    "    \n",
    "count_map_list = list(map(count, df['tweet'][:10]))\n",
    "master_count = sum(count_map_list, Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "simple_preprocess(df['tweet'][0], min_len=5,max_len=40, deacc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pprint\n",
    "import gensim.parsing as gp\n",
    "from gensim.utils import deaccent\n",
    "\n",
    "with open('stopwords-es.txt', 'r') as file:\n",
    "    stopwords = list()\n",
    "    for line in file:\n",
    "        stop_word = deaccent(line.strip('\\n'))\n",
    "        stopwords.append(stop_word)\n",
    "stopwords = frozenset(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OutputStreaming_20191101-171445.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tw_handles = r\"([@][A-z]+)|([#][A-z]+)\"\n",
    "urls = r\"((\\w+:\\/\\/)[-a-zA-Z0-9:@;?&=\\/%\\+\\.\\*!'\\(\\),\\$_\\{\\}\\^~\\[\\]`#|]+)\"\n",
    "multi_pattern = '|'.join([tw_handles, urls])\n",
    "non_plain_re = re.compile(multi_pattern, re.UNICODE)\n",
    "\n",
    "def remove_non_plain(document):\n",
    "    return non_plain_re.sub('', document)\n",
    "\n",
    "\n",
    "def strip_punctuation(token):\n",
    "    return deaccent(token).strip('\",.:;?¿-()[]<>!¡“”|*/\\=+&$% ')\n",
    "\n",
    "doc = df['text'][11]\n",
    "# filtered_tokenization = [strip_punctuation(word) for word in remove_non_plain(doc).lower().split() if word not in stopwords]\n",
    "# print(filtered_tokenization)\n",
    "\n",
    "def process(doc):\n",
    "    wordbag = list()\n",
    "    for token in remove_non_plain(doc).lower().split():\n",
    "        token = strip_punctuation(token)\n",
    "        if token not in stopwords and token != '' and token != 'rt':\n",
    "            wordbag.append(token)\n",
    "    return set(wordbag)\n",
    "\n",
    "documents = map(process, df['text'])\n",
    "tfs = Counter()\n",
    "for document in documents:\n",
    "    tfs.update(document)\n",
    "\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# dct = Dictionary(documents)\n",
    "# tfs= dict()\n",
    "# for token in dct.values():\n",
    "#     id_ = dct.token2id[token]\n",
    "#     tfs[token] = dct.cfs[id_]\n",
    "\n",
    "# tfs = Counter(tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud().generate_from_frequencies(tfs)\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from npl_utils import process, init_counter\n",
    "df = json_pandas(read_mongo('dbTweets', 'tweets_chile',\n",
    "                           query_fields={\"dateTweet\":1, \"tweet\":1, \"screenName\":1},\n",
    "                           json_only=True, num_limit=10 ** 5))\n",
    "\n",
    "twiterator = map(process, df['tweet'])\n",
    "ctmain = init_counter(twiterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctmain.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json_pandas(read_mongo('dbTweets', 'tweets_chile',\n",
    "                           query_fields={\"dateTweet\":1, \"tweet\":1, \"screenName\":1},\n",
    "                           json_only=True, num_limit=10 ** 5))\n",
    "ctmain.update(init_counter(map(process, df['tweet'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wc(counter: object, wc_kwargs: object = {\"background_color\": 'white', \"colormap\": 'plasma',\n",
    "                                                  \"width\": 1200, \"height\": 800}) -> object:\n",
    "    \"\"\"\n",
    "    Generate a wordcloud of the keywords given, wheighted by the number of\n",
    "    unique tweets they appear in. Returns a go.Figure() instance.\n",
    "\n",
    "    :param tpm: Dataframe with the words frequency per minute.\n",
    "    :param keywords: list of strings to plot in the word cloud.\n",
    "    :param wc_kwargs: dict of keyword arguments to give to the WordCloud\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "    # Build the word cloud from the data\n",
    "#     wf = get_word_frequency(tpm, keywords)\n",
    "#     new_keywords = []\n",
    "#     for key in keywords:\n",
    "#         if wf[key] > 0:\n",
    "#             new_keywords.append(key)\n",
    "\n",
    "#     keywords = new_keywords\n",
    "#     wf = {key:wf[key] for key in keywords}\n",
    "#     if len(wf) == 0:\n",
    "#         return go.Figure()\n",
    "\n",
    "    word_cloud = WordCloud(**wc_kwargs).generate_from_frequencies(counter)\n",
    "\n",
    "    wc_raster = Image.fromarray(word_cloud.to_array())\n",
    "\n",
    "    # Call the constructor of Figure object\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Constants\n",
    "    img_width = 1600\n",
    "    img_height = 900\n",
    "    scale_factor = 0.5\n",
    "\n",
    "    # Add invisible scatter trace.\n",
    "    # This trace is added to help the autoresize logic work.\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[0, img_width * scale_factor],\n",
    "            y=[0, img_height * scale_factor],\n",
    "            mode=\"markers\",\n",
    "            marker_opacity=0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Configure axes\n",
    "    fig.update_xaxes(\n",
    "        visible=False,\n",
    "        range=[0, img_width * scale_factor]\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        visible=False,\n",
    "        range=[0, img_height * scale_factor],\n",
    "        # the scaleanchor attribute ensures that the aspect ratio stays constant\n",
    "        scaleanchor=\"x\"\n",
    "    )\n",
    "\n",
    "    # Add image\n",
    "    fig.update_layout(\n",
    "        images=[go.layout.Image(\n",
    "            x=0,\n",
    "            sizex=img_width * scale_factor,\n",
    "            y=img_height * scale_factor,\n",
    "            sizey=img_height * scale_factor,\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            opacity=1.0,\n",
    "            layer=\"below\",\n",
    "            sizing=\"stretch\",\n",
    "            source=wc_raster)]\n",
    "    )\n",
    "\n",
    "    # Configure other layout\n",
    "    fig.update_layout(\n",
    "        width=img_width * scale_factor,\n",
    "        height=img_height * scale_factor,\n",
    "        margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0}\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "create_wc(dict(ctmain.most_common(35)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import strip_punctuation, strip_punctuation2\n",
    "strip_punctuation2('23197(&**Ŷ*((*jhj,sdhja<>?/|œh👽')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(df['tweet'][999])\n",
    "pprint('----')\n",
    "pprint(Counter(set(process(df['tweet'][999]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(ctmain.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hola', 'PROPN'), ('esto', 'PRON'), ('es', 'AUX'), ('una', 'DET'), ('frase', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "from es_lemmatizer import lemmatize\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "nlp.add_pipe(lemmatize, after=\"tagger\")\n",
    "doc = nlp('Hola esto es una frase')\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'parsed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cc21edfc98bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Esto es un texto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Me gustas tú'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'En el barrio La Cachimba se ha formado la corredera'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'parsed'"
     ]
    }
   ],
   "source": [
    "texts = ['Esto es un texto', 'Me gustas tú', 'En el barrio La Cachimba se ha formado la corredera']\n",
    "for doc in nlp.pipe(texts, batch_size=50):\n",
    "    print(doc.parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Esto es un texto',\n",
       " 'ents': [],\n",
       " 'sents': [{'start': 0, 'end': 16}],\n",
       " 'tokens': [{'id': 0,\n",
       "   'start': 0,\n",
       "   'end': 4,\n",
       "   'pos': 'PRON',\n",
       "   'tag': 'PRON__Number=Sing|PronType=Dem',\n",
       "   'dep': 'nsubj',\n",
       "   'head': 3},\n",
       "  {'id': 1,\n",
       "   'start': 5,\n",
       "   'end': 7,\n",
       "   'pos': 'AUX',\n",
       "   'tag': 'AUX__Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   'dep': 'cop',\n",
       "   'head': 3},\n",
       "  {'id': 2,\n",
       "   'start': 8,\n",
       "   'end': 10,\n",
       "   'pos': 'DET',\n",
       "   'tag': 'DET__Definite=Ind|Gender=Masc|Number=Sing|PronType=Art',\n",
       "   'dep': 'det',\n",
       "   'head': 3},\n",
       "  {'id': 3,\n",
       "   'start': 11,\n",
       "   'end': 16,\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NOUN__Gender=Masc|Number=Sing',\n",
       "   'dep': 'ROOT',\n",
       "   'head': 3}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = lemmatize(doc)\n",
    "lemmas.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.77085952\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.dbTweets\n",
    "\n",
    "# print collection statistics\n",
    "# events is the collection name here\n",
    "\n",
    "# print database statistics\n",
    "print(db.command(\"dbstats\")['storageSize']*1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/cemas/codigo/env/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download some functions from the nltk package if not found on the computer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/manuel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "clf = classifier.SentimentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuit número 606: Oye imbecil retardado, están mirando todo desde arriba ... anda a chuparle el pico al comunista infestado de  @gabrielboric ==> 0.3171715584959997\n"
     ]
    }
   ],
   "source": [
    "index = np.random.randint(1, high=1000)\n",
    "tweet = df['text'][index]\n",
    "print('Tuit número {}: {} ==> {}'.format(index, tweet, clf.predict(tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = json_pandas(read_mongo('TweetScraper', 'tweet',\n",
    "                           query_fields={\"datetime\":1, \"text\":1, \"usernameTweet\":1},\n",
    "                           json_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "twiterator = map(process, df['text'])\n",
    "ctmain = init_counter(twiterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('com', 9260),\n",
       " ('twitter', 8354),\n",
       " ('https', 5964),\n",
       " ('pic', 3553),\n",
       " ('status', 2137),\n",
       " ('chile', 1767),\n",
       " ('cl', 1377),\n",
       " ('gente', 1089),\n",
       " ('pinera', 795),\n",
       " ('http', 717),\n",
       " ('tatus', 681),\n",
       " ('anos', 665),\n",
       " ('gracias', 645),\n",
       " ('st', 635),\n",
       " ('atus', 629)]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctmain.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Trending Topic número uno en Chile a las 22:30:00 es: #EnChileNosDejanCiegos \n",
      "Más información en  https://www. litoralpress.cl/sitio/trending s.cshtml   … 157\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import to_unicode\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "text = df['text'][153]\n",
    "strip_multiple_whitespaces(text)\n",
    "print(text, len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Trending Topic número uno en Chile a las 22:30:00 es: #EnChileNosDejanCiegos \n",
      "Más informació s.cshtml   …\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url_wre = re.compile(r'[-a-zA-Z0-9@:%_\\+.~#?&//= ]{2,256}\\.[a-z]{2,4}\\b(\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?(\\?([-a-zA-Z0-9@:%_\\+.~#?&//=]+)|)', re.UNICODE)\n",
    "print(url_wre.sub('', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"((\\\\w+:\\\\/\\\\/)[-a-zA-Z0-9:@;?&=\\\\/%\\\\+\\\\.\\\\*!'\\\\(\\\\),\\\\$_\\\\{\\\\}\\\\^~\\\\[\\\\]`#|]+)\""
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from npl_utils import urls\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
